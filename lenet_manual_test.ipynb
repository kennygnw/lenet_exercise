{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3215eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import lenet_model\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "505e46f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"quantized_lenet5_mnist_2144.pth\"\n",
    "MODEL_NAME = \"quantized_lenet5_mnist_20250703_1003.pth\"\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45ef206",
   "metadata": {},
   "source": [
    "### LOAD WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "943ea54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gunaw\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\_utils.py:425: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    }
   ],
   "source": [
    "weights = torch.load(MODEL_SAVE_PATH)\n",
    "# print(weights.keys())  # Print the keys of the weights dictionary\n",
    "# print(weights['c1.weight'])  # Print the first element of the first channel\n",
    "# print(weights['c1.weight'][0, 0, 0, 0])  # Print the first element of the first channel\n",
    "# print(weights['c1.weight'].int_repr()[0, 0, 0, 0])  # Print the first element of the first channel\n",
    "# print(weights['c1.scale'])\n",
    "# print(weights['c1.zero_point'])\n",
    "# print(weights['c2.weight'])\n",
    "# print(weights['c2.scale'])\n",
    "# print(weights['c2.zero_point'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a67c97da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> Parameter containing:\n",
      "tensor([-0.1901, -0.1153,  0.0569, -0.3014, -0.3366, -0.0769],\n",
      "       requires_grad=True)\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "# odict_keys(['c1.weight', 'c1.bias', 'c2.weight', 'c2.bias', 'c3.weight', 'c3.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])\n",
    "# odict_keys(['quant.scale', 'quant.zero_point', 'c1.weight', 'c1.bias', 'c1.scale', 'c1.zero_point', 'c2.weight', 'c2.bias', 'c2.scale', 'c2.zero_point', 'c3.weight', 'c3.bias', 'c3.scale', 'c3.zero_point', 'fc1.scale', 'fc1.zero_point', 'fc1._packed_params.dtype', 'fc1._packed_params._packed_params', 'fc2.scale', 'fc2.zero_point', 'fc2._packed_params.dtype', 'fc2._packed_params._packed_params'])\n",
    "# print(weights.keys())\n",
    "# print(weights['quant.scale'])\n",
    "# print(weights['quant.zero_point'])\n",
    "c1_weight = weights['c1.weight']\n",
    "c1_weight.cpu()\n",
    "c1_bias = weights['c1.bias']\n",
    "c1_weight.cpu()\n",
    "print(type(c1_weight),c1_bias)\n",
    "# weights has been quantized, we dont need to quantize it again\n",
    "# print(type(weights['c1.scale'].item()))\n",
    "print(weights['c1.zero_point'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd4a2086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "           17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "           17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "           17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "           17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "           17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "           17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "           17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "           17, 17, 17, 17, 29, 43, 43, 43, 43, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 43,\n",
      "           56, 56, 56, 56, 56, 56, 56, 56, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 43, 56, 56,\n",
      "           56, 56, 56, 56, 43, 43, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 56, 56,\n",
      "           56, 43, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 56, 56, 17, 17,\n",
      "           17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 56, 56, 56, 43, 17,\n",
      "           17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 56, 56, 56, 56, 56, 56,\n",
      "           17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 56, 56, 56, 56, 56, 56, 56,\n",
      "           56, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 56, 56, 56, 17, 17, 56, 56,\n",
      "           56, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 56, 56,\n",
      "           56, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 56, 56, 43, 17, 17, 17, 17, 29, 43, 56, 56,\n",
      "           56, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56,\n",
      "           29, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 43, 56, 56, 56, 56, 56, 56, 56, 56, 56, 17,\n",
      "           17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 43, 43, 43, 43, 17, 17, 17,\n",
      "           17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "           17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "           17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "           17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "           17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "           17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "           17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "          [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "           17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17]]]], dtype=torch.uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAXX0lEQVR4nO3df2xV9f348XdVqKi0rPKjVH4I/lx0sswpIypzkYDOGVGXwGYMLgYDAzNl6sIyRZcl3VzijAvT/SUxU2EmQ6NLSBQFsg004ggxm8QyNjBS/JFwy4+BBs435/BtP1SKDGh53R+PR/JOufee9h4Op/fZc+67h7osy7IEACfYSSf6CQEgJ0AAhBAgAEIIEAAhBAiAEAIEQAgBAiCEAAEQ4pRUZvbv358++OCDNHDgwFRXVxe9OgAcpfz6Bjt27EgtLS3ppJNOqpwA5fEZOXJk9GoAcJy2bNmSRowYUTmn4PIjHwAq35Fez/ssQAsXLkxnn312OvXUU9P48ePTm2+++T99ntNuANXhSK/nfRKgJUuWpHnz5qUFCxakt99+O40bNy5NmTIlffjhh33xdABUoqwPXH755dmcOXO6bu/bty9raWnJWltbj/i5pVIpvzq3YRiGkSp75K/nX6TXj4A+/fTTtHbt2jRp0qSu+/JZEPnt1atXH7L83r17U0dHR7cBQPXr9QB9/PHHad++fWnYsGHd7s9vt7e3H7J8a2tramxs7BpmwAHUhvBZcPPnz0+lUqlr5NP2AKh+vf57QIMHD04nn3xy2rZtW7f789vNzc2HLF9fX18MAGpLrx8B9e/fP1166aVp+fLl3a5ukN+eMGFCbz8dABWqT66EkE/BnjFjRvr617+eLr/88vTYY4+lXbt2pR/84Ad98XQAVKA+CdC0adPSRx99lB588MFi4sFXv/rVtGzZskMmJgBQu+ryudipjOTTsPPZcABUtnxiWUNDQ/nOggOgNgkQACEECIAQAgRACAECIIQAARBCgAAIIUAAhBAgAEIIEAAhBAiAEAIEQAgBAiCEAAEQQoAACCFAAIQQIABCCBAAIQQIgBACBEAIAQIghAABEEKAAAghQACEECAAQggQACEECIAQAgRACAECIIQAARBCgAAIIUAAhBAgAEIIEAAhBAiAEAIEQAgBAiCEAAEQQoAACCFAAIQQIABCCBAAIQQIgBACBEAIAQIghAABEEKAAAghQACEECAAQggQACEECIAQAgRACAECIIQAARBCgACojgA99NBDqa6urtu48MILe/tpAKhwp/TFF73ooovSq6+++n9PckqfPA0AFaxPypAHp7m5uS++NABVok/eA3rvvfdSS0tLGjt2bLr11lvT5s2bD7vs3r17U0dHR7cBQPXr9QCNHz8+LVq0KC1btiw98cQTadOmTemqq65KO3bs6HH51tbW1NjY2DVGjhzZ26sEQBmqy7Is68sn2L59exo9enR69NFH0x133NHjEVA+OuVHQCIEUPlKpVJqaGg47ON9Pjtg0KBB6fzzz09tbW09Pl5fX18MAGpLn/8e0M6dO9PGjRvT8OHD+/qpAKjlAN17771p5cqV6d///nf629/+lm666aZ08sknp+9973u9/VQAVLBePwX3/vvvF7H55JNP0pAhQ9KVV16Z1qxZU/wZAE7YJISjlU9CyGfDAVDdkxBcCw6AEAIEQAgBAiCEAAEQQoAACCFAAIQQIABCCBAAIQQIgBACBEAIAQIghAABEKLP/0M6gGjXX3/9MX3ebbfdlqrN9OnTU7lwBARACAECIIQAARBCgAAIIUAAhBAgAEIIEAAhBAiAEAIEQAgBAiCEAAEQQoAACCFAAIRwNWyoYosXLz6mz5s2bVqvrwu9q66uLlU6R0AAhBAgAEIIEAAhBAiAEAIEQAgBAiCEAAEQQoAACCFAAIQQIABCCBAAIQQIgBAuRgoVcpFQFwg9dkuWLDlhzzV9+vQT9lyVzhEQACEECIAQAgRACAECIIQAARBCgAAIIUAAhBAgAEIIEAAhBAiAEAIEQAgBAiBEXZZlWSojHR0dqbGxMXo1oOyU2bdqr1zw04U7q1upVEoNDQ2HfdwREAAhBAiAygjQqlWr0g033JBaWlpSXV1deuGFFw45TfDggw+m4cOHpwEDBqRJkyal9957rzfXGYBaDNCuXbvSuHHj0sKFC3t8/JFHHkmPP/54evLJJ9Mbb7yRTj/99DRlypS0Z8+e3lhfAGr1f0S97rrritGT/OjnscceSz/72c/SjTfeWNz39NNPp2HDhhVHSt5wBKBP3gPatGlTam9vL067dcpntI0fPz6tXr26x8/Zu3dvMfPt4AFA9evVAOXxyeVHPAfLb3c+9nmtra1FpDrHyJEje3OVAChT4bPg5s+fX8wV7xxbtmyJXiUAKi1Azc3Nxcdt27Z1uz+/3fnY59XX1xe/qHTwAKD69WqAxowZU4Rm+fLlXffl7+nks+EmTJjQm08FQK3Ngtu5c2dqa2vrNvFg3bp1qampKY0aNSrdfffd6Re/+EU677zziiA98MADxe8MTZ06tbfXHYBaCtBbb72VvvWtb3XdnjdvXvFxxowZadGiRen+++8vflfozjvvTNu3b09XXnllWrZsWTr11FN7d80BqGguRgrH6UR9C+VXHoFK4mKkAJQlAQIghAABEEKAAAghQACEECAAQggQACEECIAQAgRACAECIIQAARBCgAAIIUAAVMZ/xwDVrMwuDt/N4sWLj/pzpk+f3ifrAr3BERAAIQQIgBACBEAIAQIghAABEEKAAAghQACEECAAQggQACEECIAQAgRACAECIERdVmZXX+zo6EiNjY3Rq0GFK7PduuLU1dVFrwJVoFQqpYaGhsM+7ggIgBACBEAIAQIghAABEEKAAAghQACEECAAQggQACEECIAQAgRACAECIIQAARDilJinhf+dC4tWxjZ3AVOOliMgAEIIEAAhBAiAEAIEQAgBAiCEAAEQQoAACCFAAIQQIABCCBAAIQQIgBACBEAIFyOF43SiLsLpoqxUG0dAAIQQIAAqI0CrVq1KN9xwQ2ppaSlOPbzwwgvdHr/99tuL+w8e1157bW+uMwC1GKBdu3alcePGpYULFx52mTw4W7du7RrPPffc8a4nALU+CeG6664rxhepr69Pzc3Nx7NeAFS5PnkPaMWKFWno0KHpggsuSLNnz06ffPLJYZfdu3dv6ujo6DYAqH69HqD89NvTTz+dli9fnn71q1+llStXFkdM+/bt63H51tbW1NjY2DVGjhzZ26sEQBmqy47jlwvyCQZLly5NU6dOPewy//rXv9I555yTXn311XTNNdf0eASUj075EZAIUUm//+L3gE7sdqBylEql1NDQEDcNe+zYsWnw4MGpra3tsO8X5St48ACg+vV5gN5///3iPaDhw4f39VMBUM2z4Hbu3NntaGbTpk1p3bp1qampqRgPP/xwuuWWW4pZcBs3bkz3339/Ovfcc9OUKVN6e90BqGTZUXr99dfzE9GHjBkzZmS7d+/OJk+enA0ZMiTr169fNnr06GzmzJlZe3v7//z1S6VSj1/fqN1R7myHA6L3EyOV3chfz7/IcU1C6Av5JIR8NhzQXZl9qx7CJATKbhICAPREgAAIIUAAhBAgAEIIEAAhBAiAEAIEQAgBAiCEAAEQQoAACCFAAIQQIABCCBAAlfH/AVHeTuQVk5csWXLUnzN9+vRUba6//vqj/pzbbrutT9YFKokjIABCCBAAIQQIgBACBEAIAQIghAABEEKAAAghQACEECAAQggQACEECIAQAgRAiLrsRF698n/Q0dGRGhsbo1ejYpXZPycV6jvf+c5Rf86f//znPlkXKlepVEoNDQ2HfdwREAAhBAiAEAIEQAgBAiCEAAEQQoAACCFAAIQQIABCCBAAIQQIgBACBEAIAQIgxCkxT0tfWbJkyVF/zrRp0/pkXYhXV1cXvQpwWI6AAAghQACEECAAQggQACEECIAQAgRACAECIIQAARBCgAAIIUAAhBAgAEIIEAAh6rIsy1IZ6ejoSI2NjdGrQRlZvHhxqjbTp0+PXgXoc6VSKTU0NBz2cUdAAIQQIADKP0Ctra3psssuSwMHDkxDhw5NU6dOTRs2bOi2zJ49e9KcOXPSmWeemc4444x0yy23pG3btvX2egNQSwFauXJlEZc1a9akV155JX322Wdp8uTJadeuXV3L3HPPPemll15Kzz//fLH8Bx98kG6++ea+WHcAanUSwkcffVQcCeWhmThxYvGG05AhQ9Kzzz6bvvvd7xbLvPvuu+nLX/5yWr16dfrGN75xxK9pEgKfZxICVKY+nYSQf/FcU1NT8XHt2rXFUdGkSZO6lrnwwgvTqFGjigD1ZO/evUV0Dh4AVL9jDtD+/fvT3Xffna644op08cUXF/e1t7en/v37p0GDBnVbdtiwYcVjh3tfKT/i6RwjR4481lUCoBYClL8X9M477xz36ZH58+cXR1KdY8uWLcf19QCoDKccyyfNnTs3vfzyy2nVqlVpxIgRXfc3NzenTz/9NG3fvr3bUVA+Cy5/rCf19fXFAKC2HNURUD5fIY/P0qVL02uvvZbGjBnT7fFLL7009evXLy1fvrzrvnya9ubNm9OECRN6b60BqK0joPy0Wz7D7cUXXyx+F6jzfZ38vZsBAwYUH++44440b968YmJCPvvhrrvuKuLzv8yAA6B2HFWAnnjiieLj1Vdf3e3+p556Kt1+++3Fn3/zm9+kk046qfgF1HyG25QpU9Lvfve73lxnAKqAi5EC0CdcjBSAsiRAAIQQIABCCBAAIQQIgBACBEAIAQIghAABEEKAAAghQACEECAAQggQACEECIAQAgRACAECIIQAARBCgAAIIUAAhBAgAEIIEAAhBAiAEAIEQAgBAiCEAAEQQoAACCFAAIQQIABCCBAAIQQIgBACBEAIAQIghAABEEKAAAghQACEECAAQggQACEECIAQAgRACAECIIQAARBCgAAIIUAAhBAgAEIIEAAhBAiAEAIEQAgBAiCEAAEQQoAACCFAAIQQIABCCBAAIQQIgBACBEAIAQKg/APU2tqaLrvssjRw4MA0dOjQNHXq1LRhw4Zuy1x99dWprq6u25g1a1ZvrzcAtRSglStXpjlz5qQ1a9akV155JX322Wdp8uTJadeuXd2WmzlzZtq6dWvXeOSRR3p7vQGocKcczcLLli3rdnvRokXFkdDatWvTxIkTu+4/7bTTUnNzc++tJQBV57jeAyqVSsXHpqambvc/88wzafDgweniiy9O8+fPT7t37z7s19i7d2/q6OjoNgCoAdkx2rdvX3b99ddnV1xxRbf7f//732fLli3L1q9fn/3hD3/IzjrrrOymm2467NdZsGBBlq+GYRiGkapqlEqlL+zIMQdo1qxZ2ejRo7MtW7Z84XLLly8vVqStra3Hx/fs2VOsZOfIv170RjMMwzBSnwfoqN4D6jR37tz08ssvp1WrVqURI0Z84bLjx48vPra1taVzzjnnkMfr6+uLAUBtOaoA5UdMd911V1q6dGlasWJFGjNmzBE/Z926dcXH4cOHH/taAlDbAcqnYD/77LPpxRdfLH4XqL29vbi/sbExDRgwIG3cuLF4/Nvf/nY688wz0/r169M999xTzJC75JJL+urvAEAlOpr3fQ53nu+pp54qHt+8eXM2ceLErKmpKauvr8/OPffc7L777jviecCD5ctGn7c0DMMw0nGPI7321/3/sJSNfBp2fkQFQGXLf1WnoaHhsI+7FhwAIQQIgBACBEAIAQIghAABEEKAAAghQACEECAAQggQACEECIAQAgRACAECIIQAARBCgAAIIUAAhBAgAEIIEAAhBAiAEAIEQAgBAiCEAAEQQoAACCFAAIQQIABCCBAAIcouQFmWRa8CACfg9bzsArRjx47oVQDgBLye12Vldsixf//+9MEHH6SBAwemurq6bo91dHSkkSNHpi1btqSGhoZUq2yHA2yHA2yHA2yH8tkOeVby+LS0tKSTTjr8cc4pqczkKztixIgvXCbfqLW8g3WyHQ6wHQ6wHQ6wHcpjOzQ2Nh5xmbI7BQdAbRAgAEJUVIDq6+vTggULio+1zHY4wHY4wHY4wHaovO1QdpMQAKgNFXUEBED1ECAAQggQACEECIAQFROghQsXprPPPjudeuqpafz48enNN99Mteahhx4qrg5x8LjwwgtTtVu1alW64YYbit+qzv/OL7zwQrfH83k0Dz74YBo+fHgaMGBAmjRpUnrvvfdSrW2H22+//ZD949prr03VpLW1NV122WXFlVKGDh2apk6dmjZs2NBtmT179qQ5c+akM888M51xxhnplltuSdu2bUu1th2uvvrqQ/aHWbNmpXJSEQFasmRJmjdvXjG18O23307jxo1LU6ZMSR9++GGqNRdddFHaunVr1/jLX/6Sqt2uXbuKf/P8h5CePPLII+nxxx9PTz75ZHrjjTfS6aefXuwf+QtRLW2HXB6cg/eP5557LlWTlStXFnFZs2ZNeuWVV9Jnn32WJk+eXGybTvfcc0966aWX0vPPP18sn1/a6+abb061th1yM2fO7LY/5N8rZSWrAJdffnk2Z86crtv79u3LWlpastbW1qyWLFiwIBs3blxWy/JddunSpV239+/fnzU3N2e//vWvu+7bvn17Vl9fnz333HNZrWyH3IwZM7Ibb7wxqyUffvhhsS1WrlzZ9W/fr1+/7Pnnn+9a5p///GexzOrVq7Na2Q65b37zm9mPfvSjrJyV/RHQp59+mtauXVucVjn4enH57dWrV6dak59ayk/BjB07Nt16661p8+bNqZZt2rQptbe3d9s/8mtQ5adpa3H/WLFiRXFK5oILLkizZ89On3zySapmpVKp+NjU1FR8zF8r8qOBg/eH/DT1qFGjqnp/KH1uO3R65pln0uDBg9PFF1+c5s+fn3bv3p3KSdldjPTzPv7447Rv3740bNiwbvfnt999991US/IX1UWLFhUvLvnh9MMPP5yuuuqq9M477xTngmtRHp9cT/tH52O1Ij/9lp9qGjNmTNq4cWP66U9/mq677rrihffkk09O1Sa/cv7dd9+drrjiiuIFNpf/m/fv3z8NGjSoZvaH/T1sh9z3v//9NHr06OIH1vXr16ef/OQnxftEf/rTn1K5KPsA8X/yF5NOl1xySRGkfAf74x//mO64447QdSPe9OnTu/78la98pdhHzjnnnOKo6JprrknVJn8PJP/hqxbeBz2W7XDnnXd22x/ySTr5fpD/cJLvF+Wg7E/B5YeP+U9vn5/Fkt9ubm5OtSz/Ke/8889PbW1tqVZ17gP2j0Plp2nz759q3D/mzp2bXn755fT66693++9b8n/z/LT99u3ba2J/mHuY7dCT/AfWXDntD2UfoPxw+tJLL03Lly/vdsiZ354wYUKqZTt37ix+msl/sqlV+emm/IXl4P0j/w+58tlwtb5/vP/++8V7QNW0f+TzL/IX3aVLl6bXXnut+Pc/WP5a0a9fv277Q37aKX+vtJr2h+wI26En69atKz6W1f6QVYDFixcXs5oWLVqU/eMf/8juvPPObNCgQVl7e3tWS3784x9nK1asyDZt2pT99a9/zSZNmpQNHjy4mAFTzXbs2JH9/e9/L0a+yz766KPFn//zn/8Uj//yl78s9ocXX3wxW79+fTETbMyYMdl///vfrFa2Q/7YvffeW8z0yvePV199Nfva176WnXfeedmePXuyajF79uyssbGx+D7YunVr19i9e3fXMrNmzcpGjRqVvfbaa9lbb72VTZgwoRjVZPYRtkNbW1v285//vPj75/tD/r0xduzYbOLEiVk5qYgA5X77298WO1X//v2Ladlr1qzJas20adOy4cOHF9vgrLPOKm7nO1q1e/3114sX3M+PfNpx51TsBx54IBs2bFjxg8o111yTbdiwIaul7ZC/8EyePDkbMmRIMQ159OjR2cyZM6vuh7Se/v75eOqpp7qWyX/w+OEPf5h96Utfyk477bTspptuKl6ca2k7bN68uYhNU1NT8T1x7rnnZvfdd19WKpWycuK/YwAgRNm/BwRAdRIgAEIIEAAhBAiAEAIEQAgBAiCEAAEQQoAACCFAAIQQIABCCBAAIQQIgBTh/wHAVyqLAb3U6AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# load image\n",
    "IMAGE_NAME = \"img_01.png\"\n",
    "IMAGE_PATH = Path(Path.cwd() / \"results\" / \"images\" / IMAGE_NAME)\n",
    "\n",
    "dataset_mean = 0.132515\n",
    "dataset_std = 0.310480\n",
    "quant_scale = weights['quant.scale'].item()\n",
    "quant_zero_point = weights['quant.zero_point'].item()\n",
    "\n",
    "img = Image.open(IMAGE_PATH).convert('L')  # 'L' for grayscale\n",
    "\n",
    "pix = np.array(img.getdata()).reshape(img.size[1], img.size[0]).astype(np.uint8)  # shape: [28, 28]\n",
    "plt.imshow(img, cmap='gray')\n",
    "# PyTorch pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # [H, W] → [1, H, W], uint8 → float32 in [0,1]\n",
    "    # transforms.Normalize(mean=[dataset_mean], std=[dataset_std])\n",
    "])\n",
    "img_tensor = transform(img)  # shape: [1, 28, 28], float32\n",
    "# print(img_tensor.dtype)\n",
    "img_quantized = torch.quantize_per_tensor(img_tensor, scale=quant_scale, zero_point=quant_zero_point, dtype=torch.quint8).unsqueeze(0)  # shape: [1, 1, 28, 28]\n",
    "print(img_quantized.int_repr())  # Print the quantized integer representation\n",
    "# load the image as numpy array (manual code)\n",
    "img_manual_array = np.array(img).astype(np.uint8)  # shape: [28, 28]\n",
    "# normalize the image manually\n",
    "scale = 1 / (255* dataset_std)\n",
    "offset = -dataset_mean / dataset_std\n",
    "# quantize manually\n",
    "img_int8_norm = img_manual_array.astype(np.float32) * scale + offset\n",
    "# int8_tensor = torch.round(img_tensor / quant_scale + quant_zero_point).clamp(-128, 127).to(torch.int8)\n",
    "img_int8_quantized = np.round(img_int8_norm / quant_scale + quant_zero_point)\n",
    "img_int8_quantized = np.clip(img_int8_quantized, -128, 127).astype(np.int8)\n",
    "# img_int8_quantized = np.clip(img_int8_quantized, 0, 255).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "287d18ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import pandas as pd\n",
    "# # set backend to quantized\n",
    "# torch.backends.quantized.engine = 'fbgemm'\n",
    "# # Create a quantized conv2d layer (same shape as c1)\n",
    "# conv_q = nn.quantized.Conv2d(\n",
    "#     in_channels=1,\n",
    "#     out_channels=6,\n",
    "#     kernel_size=5,\n",
    "#     stride=1,\n",
    "#     padding=2,\n",
    "#     dtype=torch.quint8\n",
    "# )\n",
    "# conv_q_non_quantized = nn.Conv2d(\n",
    "#     in_channels=1,\n",
    "#     out_channels=6,\n",
    "#     kernel_size=5,\n",
    "#     stride=1,\n",
    "#     padding=2,\n",
    "# )\n",
    "# # apply weight and bias to conv_q\n",
    "# conv_q.set_weight_bias(weights['c1.weight'], weights['c1.bias'])\n",
    "# # apply weights to conv_q_non_quantized\n",
    "# conv_q_non_quantized.weight = nn.Parameter(c1_weight.int_repr().to(torch.float32))  # convert to float32\n",
    "# conv_q_non_quantized.bias = nn.Parameter(c1_bias)\n",
    "\n",
    "# # print('conv_q_weight', conv_q.weight().int_repr())\n",
    "# # print('conv_q_non_quantized weight', conv_q_non_quantized.weight)\n",
    "# # print('weight difference when loading to layer', torch.sum(torch.abs(conv_q.weight().int_repr() - c1_weight.int_repr())))\n",
    "\n",
    "# img_int8_quantized_tensor = torch.tensor(img_int8_quantized, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(torch.float32)  # shape: [1, 1, 28, 28]\n",
    "# img_quantized = torch.quantize_per_tensor(img_tensor, scale=weights['quant.scale'], zero_point=weights['quant.zero_point'], dtype=torch.quint8).unsqueeze(0)  # shape: [1, 1, 28, 28]\n",
    "\n",
    "# # print(img_quantized.dtype)  \n",
    "# # print(img_int8_quantized_tensor.dtype)\n",
    "# # print(img_quantized.shape)  \n",
    "# # print(img_int8_quantized_tensor.shape)  \n",
    "# # print(img_quantized.int_repr()) \n",
    "# print('diff on input image', torch.sum(img_quantized.int_repr() - img_int8_quantized_tensor))  \n",
    "\n",
    "# with torch.no_grad():\n",
    "#     conv_output = conv_q(img_quantized)  # conv_q is nn.quantized.Conv2d\n",
    "#     conv_output_manual_img = conv_q_non_quantized(img_int8_quantized_tensor)  # conv_q is nn.quantized.Conv2d\n",
    "# torch.set_printoptions(profile=\"full\")\n",
    "# print(conv_output[0,0,:,:])\n",
    "# print(conv_output_manual_img[0,0,:,:].to(torch.int8))\n",
    "# # # Optional: flatten if needed\n",
    "# # y_flat = y_int8.flatten()\n",
    "# # # Save as binary or C array\n",
    "# # np.savetxt(\"conv_output.txt\", y_flat, fmt='%d', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be5d53",
   "metadata": {},
   "source": [
    "#### RUN CONV2D.PREPACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "1a8d1b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67,\n",
      "        67, 67, 67, 67, 67, 67, 67, 67, 67, 67], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# --- 1) Prepare your data and params ---\n",
    "img_quantized = torch.quantize_per_tensor(img_tensor, scale=weights['quant.scale'], zero_point=weights['quant.zero_point'], dtype=torch.quint8).unsqueeze(0)  # shape: [1, 1, 28, 28]\n",
    "# torch.save(img_quantized, 'img_01.png_quantized_manual.pt')\n",
    "# Quantized input (QUInt8)\n",
    "# x_fp32: float32 tensor [1,1,H,W]\n",
    "x_fp32 = img_tensor\n",
    "x_q = img_quantized\n",
    "# Quantized weight (QInt8) and float bias\n",
    "w_q = weights['c1.weight']         # torch.qint8\n",
    "b_fp32 = weights['c1.bias']        # torch.float32\n",
    "# b_fp32 = torch.tensor([0,0,0,0,0,0], dtype=torch.float32)  # Use zero bias for simplicity\n",
    "\n",
    "# Stride/pad/dilation/groups\n",
    "stride = (1, 1)\n",
    "padding = (2, 2)\n",
    "dilation = (1, 1)\n",
    "groups = 1\n",
    "\n",
    "# The output quant params (from your model)\n",
    "y_scale = weights['c1.scale']  \n",
    "y_zp    = weights['c1.zero_point']\n",
    "# --- 2) Pack the weights & bias once --- \n",
    "packed_w = torch.ops.quantized.conv2d_prepack(\n",
    "    w_q, b_fp32,\n",
    "    stride, padding, dilation, groups\n",
    ")\n",
    "\n",
    "# --- 3) Do the quantized conv2d --- \n",
    "y_q = torch.ops.quantized.conv2d(\n",
    "    x_q,                # QUInt8 input\n",
    "    packed_w,           # packed QInt8 weight + bias\n",
    "    y_scale, y_zp       # output scale & zero_point\n",
    ")\n",
    "\n",
    "# y_q is a torch.quint8 tensor: it’s the exact same as conv_q(x_q)\n",
    "# print(y_q.shape, y_q.dtype, y_q.q_scale(), y_q.q_zero_point())\n",
    "# print(y_q[0,0,0,:])\n",
    "print(y_q.int_repr()[0,0,0,:])  # raw uint8 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "bed2b645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5(\n",
      "  (quant): QuantStub(\n",
      "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "  )\n",
      "  (dequant): DeQuantStub()\n",
      "  (c1): Conv2d(\n",
      "    1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)\n",
      "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "    )\n",
      "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "  )\n",
      "  (c2): Conv2d(\n",
      "    6, 16, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "    )\n",
      "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "  )\n",
      "  (c3): Conv2d(\n",
      "    16, 120, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "    )\n",
      "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "  )\n",
      "  (relu): ReLU()\n",
      "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(\n",
      "    in_features=120, out_features=84, bias=True\n",
      "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "    )\n",
      "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "  )\n",
      "  (fc2): Linear(\n",
      "    in_features=84, out_features=10, bias=True\n",
      "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "    )\n",
      "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Re-create model and set QAT config\n",
    "model_fp32 = lenet_model.LeNet5()\n",
    "model_fp32.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "torch.quantization.prepare_qat(model_fp32 , inplace=True)\n",
    "print(model_fp32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d540b26",
   "metadata": {},
   "source": [
    "### TRY TO CREATE MANUAL FIXEDPOINT CONV BASED OFF FBGEMM HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48949ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def requantize_fixed_point(src: np.ndarray, multiplier: int, right_shift: int, zero_point: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    src: int32 numpy array (accumulator values)\n",
    "    multiplier: fixed-point multiplier (int)\n",
    "    right_shift: number of bits to right-shift\n",
    "    zero_point: output zero point (usually uint8)\n",
    "    \"\"\"\n",
    "    src = src.astype(np.int64)  # promote to avoid overflow in multiplication\n",
    "    ab_64 = src * multiplier\n",
    "\n",
    "    # Add rounding offset (nudge)\n",
    "    nudge = 1 << max(0, right_shift - 1)\n",
    "    ab_64_rounded = ab_64 + nudge\n",
    "\n",
    "    # Perform requantization (right shift + zero point offset)\n",
    "    quantized_down = zero_point + (ab_64_rounded >> right_shift)\n",
    "    # Clip to uint8 range\n",
    "    \n",
    "    quantized_clipped = torch.clip(torch.tensor(quantized_down), 0, 255)\n",
    "\n",
    "    return quantized_clipped.to(torch.uint8)\n",
    "\n",
    "\n",
    "def int8_conv2d_integer_only(\n",
    "    x_q,                # torch.quint8 input  [B, C_in, H, W]\n",
    "    w_q,                # torch.qint8 weights [C_out, C_in, kH, kW]\n",
    "    b_int32,            # torch.int32 bias   [C_out]\n",
    "    x_zp, w_zp, z_y,    # zero points (ints)\n",
    "    M, R,               # per‐channel multiplier (int32) & shift (int)\n",
    "    stride=1,           # int or (stride_h, stride_w)\n",
    "    padding=0           # int or (pad_h, pad_w)\n",
    "):\n",
    "    # --- unpack stride & padding ---\n",
    "    if isinstance(stride, int):\n",
    "        stride_h = stride_w = stride\n",
    "    else:\n",
    "        stride_h, stride_w = stride\n",
    "    if isinstance(padding, int):\n",
    "        pad_h = pad_w = padding\n",
    "    else:\n",
    "        pad_h, pad_w = padding\n",
    "\n",
    "    B, C_in, H, W = x_q.shape\n",
    "    C_out, _, kH, kW = w_q.shape\n",
    "\n",
    "    # --- dequantized ints ---\n",
    "    # x_int: int32, shape [B, C_in, H, W]\n",
    "    x_int = x_q.int().to(torch.int32) - x_zp\n",
    "    # print(x_int)\n",
    "    # pad on H and W dims: (left, right, top, bottom)\n",
    "    x_int = F.pad(x_int, (pad_w, pad_w, pad_h, pad_h))\n",
    "\n",
    "    # prepare weights once\n",
    "    # w_int: int32, shape [C_out, C_in, kH, kW]\n",
    "    w_int = w_q.int_repr().to(torch.int32) - w_zp\n",
    "    # print(w_int)\n",
    "    # compute output spatial dims\n",
    "    H_p, W_p = H + 2*pad_h, W + 2*pad_w\n",
    "    H_out = (H_p - kH) // stride_h + 1\n",
    "    W_out = (W_p - kW) // stride_w + 1\n",
    "\n",
    "    # allocate output (int32 accumulator)\n",
    "    y_int32 = torch.zeros((B, C_out, H_out, W_out), dtype=torch.int32)\n",
    "\n",
    "    for b in range(B):\n",
    "        for oc in range(C_out):\n",
    "            m = M[oc].item()\n",
    "            params = {\n",
    "                'multiplier': m,        # example Q31 multiplier\n",
    "                'right_shift': R,              # shift to get back to int8 range\n",
    "                'zero_point': z_y               # typical for quantized uint8\n",
    "            }\n",
    "            for i in range(H_out):\n",
    "                for j in range(W_out):\n",
    "                    acc = np.int32(0)\n",
    "                    start_i = i * stride_h\n",
    "                    start_j = j * stride_w\n",
    "                    for ic in range(C_in):\n",
    "                        # vectorized inner loop if you like\n",
    "                        for ki in range(kH):\n",
    "                            for kj in range(kW):\n",
    "                                xv = x_int[b, ic, start_i+ki, start_j+kj].item()\n",
    "                                wv = w_int[oc, ic, ki, kj].item()\n",
    "                                acc += xv * wv\n",
    "                    acc += b_int32[oc].item()\n",
    "\n",
    "                    y_int32[b,oc,i,j] = requantize_fixed_point(acc, **params)\n",
    "                # print('\\n')\n",
    "    # clamp to [0,255] and cast to uint8\n",
    "    y_uint8 = y_int32.clamp(0, 255).to(torch.uint8)\n",
    "    return y_uint8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c766743d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplier tensor(0.0008)\n",
      "max_m 0.0007882966310717165\n",
      "INT32_MAX 2147483647\n",
      "R_max 41\n",
      "Using R = 31\n",
      "First few fixed-point multipliers: tensor(1692854, dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gunaw\\AppData\\Local\\Temp\\ipykernel_15932\\3416531865.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  quantized_clipped = torch.clip(torch.tensor(quantized_down), 0, 255)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "img_int8_quantized_tensor = torch.tensor(img_int8_quantized, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(torch.float32)  # shape: [1, 1, 28, 28]\n",
    "\n",
    "bias_int32 = weights['c1.bias'] / (weights['quant.scale'].item()  * weights['c1.weight'].q_scale()) # convert bias to int32\n",
    "bias_int32 = torch.round(bias_int32).to(torch.int32)\n",
    "# bias_int32 = torch.tensor([0,0,0,0,0,0], dtype=torch.int32)  # Use zero bias for simplicity\n",
    "# print(bias_int32)\n",
    "scale_weight = torch.tensor(weights['c1.weight'].q_scale())\n",
    "scale_x = weights['quant.scale'].item()  # scale of input\n",
    "scale_y = weights['c1.scale'].item()  # scale of output\n",
    "# --- 2) Compute the “real” per-channel multiplier m_o ------------\n",
    "#   m_o = (s_x * s_w[o]) / s_y\n",
    "multipler = scale_weight.mul_(scale_x / scale_y)   # in-place to save memory, shape [C_out]\n",
    "print('multiplier', multipler)\n",
    "\n",
    "# --- 3) Choose R so that max(|M[o]|) <= 2^31 - 1 ------------------\n",
    "# We need M[o] = round(m[o] * 2^R) to fit in int32.\n",
    "# So R <= floor( log2((2^31 - 1) / max(|m[o]|)) )\n",
    "max_m = multipler.abs().max().item()\n",
    "print('max_m', multipler.abs().max().item())\n",
    "INT32_MAX = 2**31 - 1\n",
    "print('INT32_MAX', INT32_MAX)\n",
    "R_max = math.floor(math.log2(INT32_MAX / max_m))\n",
    "print('R_max', R_max)\n",
    "# clamp R to [0, 31] if you like\n",
    "R = min(31, max(0, R_max))\n",
    "# --- 4) Compute the integer multipliers M[o] ----------------------\n",
    "#   M[o] = round(m[o] * 2^R)\n",
    "M = torch.round(multipler * (2**R)).to(torch.int32)  # Tensor[C_out]\n",
    "print(f\"Using R = {R}\")\n",
    "print(\"First few fixed-point multipliers:\", M)\n",
    "M_vec = M.repeat(c1_weight.shape[0])        \n",
    "conv2d_result_manual = int8_conv2d_integer_only(\n",
    "    img_quantized.int_repr(),\n",
    "    # img_int8_quantized_tensor,\n",
    "    c1_weight,\n",
    "    bias_int32, \n",
    "    weights['quant.zero_point'].item(), weights['c1.weight'].q_zero_point(), weights['c1.zero_point'],\n",
    "    M=M_vec, R=R,\n",
    "    stride=1,padding=2)\n",
    "print(conv2d_result_manual.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "841c01f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(68)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67,\n",
       "         67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
       "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67,\n",
       "         67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
       "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67,\n",
       "         67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
       "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67,\n",
       "         67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
       "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67,\n",
       "         67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
       "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67,\n",
       "         67, 68, 68, 69, 70, 70, 69, 68, 67, 67],\n",
       "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 68, 69, 70, 71,\n",
       "         71, 71, 71, 71, 73, 73, 72, 70, 68, 67],\n",
       "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 68, 69, 71, 72, 73, 75,\n",
       "         76, 75, 73, 71, 70, 69, 71, 71, 69, 68],\n",
       "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 68, 69, 69, 72, 71, 70,\n",
       "         70, 69, 68, 66, 65, 65, 66, 69, 69, 68],\n",
       "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 68, 67, 67, 64, 63, 65, 64,\n",
       "         64, 63, 62, 63, 63, 64, 63, 66, 67, 67],\n",
       "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 68, 70, 71, 69, 67, 60, 57, 60,\n",
       "         62, 63, 62, 63, 63, 64, 65, 66, 67, 67],\n",
       "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 68, 70, 69, 71, 71, 72, 70, 65, 61,\n",
       "         62, 65, 65, 65, 66, 67, 67, 67, 67, 67],\n",
       "        [67, 67, 67, 67, 67, 67, 67, 67, 68, 70, 69, 69, 68, 69, 73, 76, 75, 70,\n",
       "         65, 66, 67, 67, 67, 67, 67, 67, 67, 67],\n",
       "        [67, 67, 67, 67, 67, 67, 67, 67, 69, 67, 69, 69, 65, 63, 65, 73, 76, 77,\n",
       "         71, 66, 67, 67, 67, 67, 67, 67, 67, 67],\n",
       "        [67, 67, 67, 67, 67, 67, 67, 67, 66, 63, 63, 63, 63, 62, 59, 65, 71, 75,\n",
       "         73, 68, 67, 67, 67, 67, 67, 67, 67, 67],\n",
       "        [67, 67, 67, 67, 68, 69, 71, 70, 65, 62, 56, 59, 63, 64, 61, 60, 66, 71,\n",
       "         73, 68, 67, 67, 67, 67, 67, 67, 67, 67],\n",
       "        [67, 67, 67, 67, 69, 69, 74, 76, 75, 70, 64, 65, 66, 66, 65, 61, 64, 69,\n",
       "         71, 69, 67, 67, 67, 67, 67, 67, 67, 67],\n",
       "        [67, 67, 67, 67, 67, 65, 68, 75, 79, 78, 76, 72, 71, 71, 68, 63, 63, 66,\n",
       "         70, 69, 67, 67, 67, 67, 67, 67, 67, 67],\n",
       "        [67, 67, 67, 67, 65, 62, 59, 64, 69, 71, 72, 72, 70, 70, 65, 64, 64, 65,\n",
       "         69, 68, 67, 67, 67, 67, 67, 67, 67, 67],\n",
       "        [67, 67, 67, 67, 66, 62, 56, 55, 58, 59, 62, 64, 63, 63, 63, 63, 64, 66,\n",
       "         67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
       "        [67, 67, 67, 67, 68, 66, 62, 60, 59, 58, 56, 57, 60, 61, 63, 62, 65, 66,\n",
       "         67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
       "        [67, 67, 67, 67, 68, 68, 67, 66, 65, 64, 62, 62, 63, 63, 64, 65, 66, 67,\n",
       "         67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
       "        [67, 67, 67, 67, 67, 67, 67, 67, 68, 67, 67, 66, 65, 66, 67, 67, 67, 67,\n",
       "         67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
       "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67,\n",
       "         67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
       "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67,\n",
       "         67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
       "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67,\n",
       "         67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
       "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67,\n",
       "         67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
       "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67,\n",
       "         67, 67, 67, 67, 67, 67, 67, 67, 67, 67]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(weights['c1.zero_point'])\n",
    "conv2d_result_manual[0,5,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9c0a9a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 28, 28])\n",
      "tensor([17])\n",
      "channel 0 tensor(0., dtype=torch.float64)\n",
      "channel 3 tensor(0., dtype=torch.float64)\n",
      "channel 5 tensor(0., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(conv2d_result_manual.shape)\n",
    "print(weights['quant.zero_point'])\n",
    "channel_0 = torch.tensor(pd.read_csv('c1_result_0_c.csv', header=None).values)[:,:28]\n",
    "print('channel 0' ,torch.sum(channel_0.unsqueeze(0).unsqueeze(0) - conv2d_result_manual[0,0,:,:]))\n",
    "# channel_1 = torch.tensor(pd.read_csv('c1_result_1_c.csv', header=None).values)\n",
    "# print(channel_1.unsqueeze(0).unsqueeze(0) - conv2d_result_manual[0,1,:,:])\n",
    "# channel_2 = torch.tensor(pd.read_csv('c1_result_2_c.csv', header=None).values)\n",
    "# print(torch.sum(channel_2.unsqueeze(0).unsqueeze(0) - conv2d_result_manual[0,2,:,:]))\n",
    "channel_3 = torch.tensor(pd.read_csv('c1_result_3_c.csv', header=None).values)[:,:28]\n",
    "print('channel 3' ,torch.sum(channel_3.unsqueeze(0).unsqueeze(0) - conv2d_result_manual[0,3,:,:]))\n",
    "channel_5 = torch.tensor(pd.read_csv('c1_result_5_c.csv', header=None).values)[:,:28]\n",
    "print('channel 5' , torch.sum(channel_5.unsqueeze(0).unsqueeze(0) - conv2d_result_manual[0,5,:,:]))\n",
    "# channel_5_no_zp = torch.tensor(pd.read_csv('c1_result_5_c_no_zp.csv', header=None).values)\n",
    "# print(channel_5_no_zp+weights['c1.zero_point']- channel_5)\n",
    "# print(torch.sum(channel_5_no_zp.unsqueeze(0).unsqueeze(0) - conv2d_result_manual[0,5,:,:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05a7eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplier 1692854\n",
    "# acc 38890\n",
    "# prod 1410582620\n",
    "# nudge 32768\n",
    "# rounded 1410615388\n",
    "# requant 1004632"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "441559d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32_t bias_int32[6] = {-1842, -1117, 551, -2920, -3261, -745};\n"
     ]
    }
   ],
   "source": [
    "arr = bias_int32.numpy().astype(np.int32)\n",
    "c_array = ', '.join(str(x) for x in arr)\n",
    "print(f\"int32_t bias_int32[{arr.size}] = {{{c_array}}};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae4057d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 28, 28])\n",
      "tensor([[67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 65],\n",
      "        [67, 67, 67, 67, 64, 65, 68, 66, 64, 65],\n",
      "        [67, 67, 67, 64, 64, 67, 68, 68, 70, 73],\n",
      "        [67, 67, 65, 65, 69, 73, 70, 73, 77, 75]], dtype=torch.uint8)\n",
      "tensor([[67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 65],\n",
      "        [67, 67, 67, 67, 64, 65, 68, 66, 64, 65],\n",
      "        [67, 67, 67, 64, 64, 67, 68, 68, 70, 73],\n",
      "        [67, 67, 65, 65, 69, 73, 70, 73, 77, 75]], dtype=torch.uint8)\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "print(conv2d_result_manual.shape)\n",
    "print(conv2d_result_manual[0,0,0:10,0:10])\n",
    "\n",
    "print(y_q.int_repr()[0,0,0:10,0:10])\n",
    "print(torch.sum(y_q.int_repr() - conv2d_result_manual))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "726b106f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_05.png_c1.pt\n"
     ]
    }
   ],
   "source": [
    "# torch.save(conv2d_result_manual, f'{IMAGE_NAME}_c1.pt')\n",
    "print(f'{IMAGE_NAME}_c1.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7100ac6a",
   "metadata": {},
   "source": [
    "### create manual maxpool2d function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "90d02954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool2d(input_array, kernel_size=2, stride=2):\n",
    "    \"\"\"\n",
    "    Performs 2D max pooling on a 4D input tensor.\n",
    "\n",
    "    Parameters:\n",
    "        input_array: np.ndarray with shape (N, C, H, W)\n",
    "        kernel_size: int or tuple of (kh, kw)\n",
    "        stride: int or tuple of (sh, sw)\n",
    "\n",
    "    Returns:\n",
    "        output_array: np.ndarray with shape (N, C, H_out, W_out)\n",
    "    \"\"\"\n",
    "    if isinstance(kernel_size, int):\n",
    "        kh, kw = kernel_size, kernel_size\n",
    "    else:\n",
    "        kh, kw = kernel_size\n",
    "\n",
    "    if isinstance(stride, int):\n",
    "        sh, sw = stride, stride\n",
    "    else:\n",
    "        sh, sw = stride\n",
    "\n",
    "    N, C, H, W = input_array.shape\n",
    "    H_out = (H - kh) // sh + 1\n",
    "    W_out = (W - kw) // sw + 1\n",
    "\n",
    "    output = np.zeros((N, C, H_out, W_out), dtype=input_array.dtype)\n",
    "\n",
    "    for n in range(N):\n",
    "        for c in range(C):\n",
    "            for i in range(H_out):\n",
    "                for j in range(W_out):\n",
    "                    h_start = i * sh\n",
    "                    h_end = h_start + kh\n",
    "                    w_start = j * sw\n",
    "                    w_end = w_start + kw\n",
    "                    window = input_array[n, c, h_start:h_end, w_start:w_end]\n",
    "                    output[n, c, i, j] = np.max(window)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a279059a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare maxpool results 0\n"
     ]
    }
   ],
   "source": [
    "c1_maxpool = maxpool2d(conv2d_result_manual.numpy(), kernel_size=2, stride=2)\n",
    "c1_maxpool_pytorch = F.max_pool2d(\n",
    "    conv2d_result_manual, kernel_size=2, stride=2)\n",
    "print('compare maxpool results', np.sum(c1_maxpool - c1_maxpool_pytorch.numpy()))\n",
    "# print(c1_maxpool)\n",
    "# torch.save(torch.tensor(c1_maxpool), f'{IMAGE_NAME}_pool_c1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c899bc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., dtype=torch.float64)\n",
      "tensor(0., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "c1mp_channel_0 = torch.tensor(pd.read_csv('c1_maxpool_0_c.csv', header=None).values)[:,:14]\n",
    "print(torch.sum(c1mp_channel_0.unsqueeze(0) - c1_maxpool_pytorch[0,0,:,:]))\n",
    "c1mp_channel_4 = torch.tensor(pd.read_csv('c1_maxpool_4_c.csv', header=None).values)[:,:14]\n",
    "print(torch.sum(c1mp_channel_4.unsqueeze(0) - c1_maxpool_pytorch[0,4,:,:]))\n",
    "# print('channel 0' ,torch.sum(channel_0.unsqueeze(0).unsqueeze(0) - conv2d_result_manual[0,0,:,:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f4d826a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (torch.tensor(c1_maxpool), f'{IMAGE_NAME}_pool_c1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716f73ed",
   "metadata": {},
   "source": [
    "### RELU FROM MAX(0,X) MAX(ZERO_POINT,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "5b661914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 72, 72, 72, 72, 71, 68, 68, 68, 68],\n",
      "          [68, 68, 73, 73, 77, 77, 74, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 71, 73, 72, 68, 68, 69, 69, 69, 70, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 70, 71, 70, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 71, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 69, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 69, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 69, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 69, 68, 68, 68, 68, 68, 68, 68]],\n",
      "\n",
      "         [[68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 69, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 70, 71, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 69, 69, 69, 68, 68, 70, 73, 68, 68, 68],\n",
      "          [68, 68, 68, 70, 71, 69, 69, 68, 68, 68, 75, 68, 68, 68],\n",
      "          [68, 68, 68, 69, 68, 68, 68, 68, 68, 69, 75, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 72, 76, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 74, 75, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 76, 73, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 76, 69, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 74, 77, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 76, 72, 68, 68, 68, 68]],\n",
      "\n",
      "         [[68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 71, 79, 78, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 69, 77, 80, 80, 82, 82, 75, 68, 68, 68, 68, 68],\n",
      "          [68, 74, 79, 81, 78, 73, 70, 71, 70, 68, 69, 68, 68, 68],\n",
      "          [68, 70, 72, 71, 69, 68, 68, 70, 69, 68, 70, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 70, 69, 68, 71, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 69, 68, 68, 70, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 71, 71, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 73, 69, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 70, 72, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 73, 75, 75, 71, 68, 68, 68, 68]],\n",
      "\n",
      "         [[68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 69, 68, 68, 68, 68, 68, 72, 72, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 69, 68, 68, 68, 68, 68, 73, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 72, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 69, 70, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 70, 70, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 69, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 70, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 69, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 69, 68, 68, 68, 68, 68, 68]],\n",
      "\n",
      "         [[68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 71, 69, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 70, 72, 68, 68, 68, 68, 68, 72, 69, 68, 68, 68, 68],\n",
      "          [68, 69, 71, 68, 68, 68, 68, 68, 75, 71, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 76, 71, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 76, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 69, 75, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 71, 75, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 74, 75, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 75, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 71, 68, 68, 68, 68, 68, 68]],\n",
      "\n",
      "         [[68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 70, 71, 73, 76, 76, 75, 72, 68, 68, 68, 68],\n",
      "          [68, 69, 72, 75, 72, 71, 69, 71, 76, 77, 69, 68, 68, 68],\n",
      "          [68, 69, 68, 68, 68, 68, 68, 68, 68, 76, 73, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 71, 72, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 69, 72, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 69, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 69, 68, 71, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 68, 71, 69, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 69, 68, 68, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 70, 68, 68, 68, 68, 68],\n",
      "          [68, 68, 68, 68, 68, 68, 68, 68, 69, 69, 68, 68, 68, 68]]]],\n",
      "       dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "c1_relu = torch.clamp(torch.tensor(c1_maxpool), min=weights['c1.zero_point'])\n",
    "\n",
    "print(c1_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "65729382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(c1_relu, f'{IMAGE_NAME}_relu_c1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581f512f",
   "metadata": {},
   "source": [
    "### PREP C2 CONV FUNCTION ON PYTORCH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "e58f89bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67.],\n",
      "          [67., 67., 67., 68., 67., 72., 72., 72., 72., 71., 67., 67., 67., 67.],\n",
      "          [67., 67., 73., 73., 77., 77., 74., 67., 67., 67., 67., 67., 67., 67.],\n",
      "          [67., 71., 73., 72., 62., 66., 69., 69., 69., 70., 67., 67., 67., 67.],\n",
      "          [67., 67., 62., 68., 70., 71., 70., 67., 67., 68., 66., 67., 67., 67.],\n",
      "          [67., 67., 68., 71., 68., 67., 67., 67., 68., 66., 66., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 67., 67., 66., 65., 67., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 67., 66., 68., 66., 68., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 67., 68., 69., 65., 67., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 67., 69., 67., 67., 68., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 67., 69., 63., 68., 67., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 69., 67., 66., 68., 67., 67., 67., 67.]],\n",
      "\n",
      "         [[67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 68., 67., 66., 66., 66., 67., 69., 67., 67., 67.],\n",
      "          [67., 68., 68., 65., 67., 61., 67., 66., 63., 70., 71., 67., 67., 67.],\n",
      "          [67., 67., 61., 66., 69., 69., 69., 66., 62., 70., 73., 67., 67., 67.],\n",
      "          [67., 67., 65., 70., 71., 69., 69., 67., 60., 68., 75., 67., 67., 67.],\n",
      "          [67., 67., 67., 69., 68., 67., 67., 67., 60., 69., 75., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 67., 67., 60., 72., 76., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 67., 67., 60., 74., 75., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 67., 68., 56., 76., 73., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 68., 66., 65., 76., 69., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 68., 62., 74., 77., 67., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 67., 62., 76., 72., 67., 67., 67., 67.]],\n",
      "\n",
      "         [[68., 68., 68., 68., 68., 68., 68., 68., 68., 68., 68., 68., 68., 68.],\n",
      "          [68., 68., 68., 68., 68., 68., 68., 68., 68., 68., 68., 68., 68., 68.],\n",
      "          [68., 68., 68., 68., 68., 68., 68., 68., 68., 68., 68., 68., 68., 68.],\n",
      "          [68., 68., 68., 68., 68., 66., 61., 58., 61., 65., 68., 68., 68., 68.],\n",
      "          [68., 68., 64., 58., 58., 71., 79., 78., 68., 61., 68., 68., 68., 68.],\n",
      "          [68., 68., 69., 77., 80., 80., 82., 82., 75., 62., 68., 68., 68., 68.],\n",
      "          [68., 74., 79., 81., 78., 73., 70., 71., 70., 65., 69., 68., 68., 68.],\n",
      "          [68., 70., 72., 71., 69., 68., 68., 70., 69., 67., 70., 68., 68., 68.],\n",
      "          [68., 68., 68., 68., 68., 68., 68., 70., 69., 68., 71., 68., 68., 68.],\n",
      "          [68., 68., 68., 68., 68., 68., 68., 69., 67., 68., 70., 68., 68., 68.],\n",
      "          [68., 68., 68., 68., 68., 68., 68., 68., 66., 71., 71., 68., 68., 68.],\n",
      "          [68., 68., 68., 68., 68., 68., 68., 67., 67., 73., 69., 68., 68., 68.],\n",
      "          [68., 68., 68., 68., 68., 68., 68., 68., 70., 72., 68., 68., 68., 68.],\n",
      "          [68., 68., 68., 68., 68., 68., 73., 75., 75., 71., 68., 68., 68., 68.]],\n",
      "\n",
      "         [[66., 66., 66., 66., 66., 66., 66., 66., 66., 66., 66., 66., 66., 66.],\n",
      "          [66., 66., 66., 66., 66., 66., 66., 66., 66., 66., 66., 66., 66., 66.],\n",
      "          [66., 66., 66., 66., 66., 66., 66., 66., 66., 66., 66., 66., 66., 66.],\n",
      "          [66., 66., 67., 66., 66., 67., 67., 66., 67., 66., 66., 66., 66., 66.],\n",
      "          [66., 66., 67., 68., 67., 67., 64., 64., 66., 64., 66., 66., 66., 66.],\n",
      "          [66., 66., 69., 65., 64., 67., 67., 68., 72., 72., 64., 66., 66., 66.],\n",
      "          [66., 65., 68., 69., 68., 67., 67., 66., 68., 73., 63., 66., 66., 66.],\n",
      "          [66., 66., 67., 67., 67., 66., 66., 66., 68., 72., 63., 66., 66., 66.],\n",
      "          [66., 66., 66., 66., 66., 66., 66., 66., 69., 70., 65., 66., 66., 66.],\n",
      "          [66., 66., 66., 66., 66., 66., 66., 65., 70., 70., 66., 66., 66., 66.],\n",
      "          [66., 66., 66., 66., 66., 66., 66., 64., 69., 66., 66., 66., 66., 66.],\n",
      "          [66., 66., 66., 66., 66., 66., 66., 66., 70., 64., 67., 66., 66., 66.],\n",
      "          [66., 66., 66., 66., 66., 66., 65., 68., 69., 67., 66., 66., 66., 66.],\n",
      "          [66., 66., 66., 66., 66., 66., 66., 69., 68., 66., 66., 66., 66., 66.]],\n",
      "\n",
      "         [[65., 65., 65., 65., 65., 65., 65., 65., 65., 65., 65., 65., 65., 65.],\n",
      "          [65., 65., 65., 65., 65., 65., 65., 65., 65., 65., 65., 65., 65., 65.],\n",
      "          [65., 65., 65., 65., 65., 65., 65., 65., 65., 65., 65., 65., 65., 65.],\n",
      "          [65., 65., 66., 66., 67., 68., 67., 65., 65., 63., 65., 65., 65., 65.],\n",
      "          [65., 66., 71., 69., 67., 68., 65., 66., 68., 63., 65., 65., 65., 65.],\n",
      "          [65., 70., 72., 67., 65., 66., 65., 68., 72., 69., 61., 65., 65., 65.],\n",
      "          [65., 69., 71., 65., 64., 65., 65., 68., 75., 71., 58., 65., 65., 65.],\n",
      "          [65., 67., 67., 65., 65., 65., 65., 67., 76., 71., 60., 65., 65., 65.],\n",
      "          [65., 65., 65., 65., 65., 65., 65., 67., 76., 68., 63., 65., 65., 65.],\n",
      "          [65., 65., 65., 65., 65., 65., 65., 69., 75., 65., 65., 65., 65., 65.],\n",
      "          [65., 65., 65., 65., 65., 65., 65., 71., 75., 60., 65., 65., 65., 65.],\n",
      "          [65., 65., 65., 65., 65., 65., 65., 74., 75., 61., 65., 65., 65., 65.],\n",
      "          [65., 65., 65., 65., 65., 65., 67., 75., 67., 65., 65., 65., 65., 65.],\n",
      "          [65., 65., 65., 65., 65., 65., 68., 71., 64., 65., 65., 65., 65., 65.]],\n",
      "\n",
      "         [[67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67.],\n",
      "          [67., 67., 68., 70., 71., 73., 76., 76., 75., 72., 67., 67., 67., 67.],\n",
      "          [67., 69., 72., 75., 72., 71., 69., 71., 76., 77., 69., 67., 67., 67.],\n",
      "          [67., 69., 68., 68., 67., 63., 65., 64., 65., 76., 73., 67., 67., 67.],\n",
      "          [67., 67., 62., 64., 66., 67., 67., 68., 63., 71., 72., 67., 67., 67.],\n",
      "          [67., 68., 67., 67., 67., 67., 67., 67., 64., 69., 72., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 67., 67., 63., 68., 69., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 67., 69., 66., 71., 68., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 68., 68., 67., 71., 69., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 69., 67., 66., 68., 67., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 68., 66., 70., 68., 67., 67., 67., 67.],\n",
      "          [67., 67., 67., 67., 67., 67., 67., 63., 69., 69., 67., 67., 67., 67.]]]],\n",
      "       size=(1, 6, 14, 14), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=1.0, zero_point=0)\n"
     ]
    }
   ],
   "source": [
    "# Quantized weight (QInt8) and float bias\n",
    "c2_w_q = weights['c2.weight']         # torch.qint8\n",
    "c2_b_fp32 = weights['c2.bias']        # torch.float32\n",
    "# Stride/pad/dilation/groups\n",
    "c2_stride = (1, 1)\n",
    "c2_padding = (0, 0)\n",
    "c2_dilation = (1, 1)\n",
    "c2_groups = 1\n",
    "\n",
    "# The output quant params (from your model)\n",
    "c2_y_scale = weights['c2.scale']  \n",
    "c2_y_zp    = weights['c2.zero_point']\n",
    "# --- 2) Pack the weights & bias once --- \n",
    "c2_packed_w = torch.ops.quantized.conv2d_prepack(\n",
    "    c2_w_q, c2_b_fp32,\n",
    "    c2_stride, c2_padding, c2_dilation, c2_groups\n",
    ")\n",
    "# Assuming you have quantized uint8 data already (no float conversion) \n",
    "c1_maxpool_quint8_wrapped = torch._empty_affine_quantized(c1_maxpool.shape, scale=1, zero_point=0, dtype=torch.quint8)\n",
    "c1_maxpool_quint8_wrapped.copy_(torch.from_numpy(np.float32(c1_maxpool)))\n",
    "# torch.quantize_per_tensor(img_tensor, scale=weights['c1.scale'], zero_point=weights['c1.zero_point'], dtype=torch.quint8).unsqueeze(0)\n",
    "# --- 3) Do the quantized conv2d --- \n",
    "print(c1_maxpool_quint8_wrapped)\n",
    "c2_q_pytorch = torch.ops.quantized.conv2d(\n",
    "    c1_maxpool_quint8_wrapped,                # QUInt8 input\n",
    "    c2_packed_w,           # packed QInt8 weight + bias\n",
    "    c2_y_scale, c2_y_zp       # output scale & zero_point\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3451286a",
   "metadata": {},
   "source": [
    "# Print weights and size of the model after quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "914cfb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using R = 31\n",
      "First few fixed-point multipliers: tensor(4544963, dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gunaw\\AppData\\Local\\Temp\\ipykernel_16680\\613243976.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  quantized_clipped = torch.clip(torch.tensor(quantized_down), 0, 255)\n"
     ]
    }
   ],
   "source": [
    "c2_bias_int32 = torch.round(weights['c2.bias'] / (weights['c1.scale'].item()  * weights['c2.weight'].q_scale())).to(torch.int32) # convert bias to int32\n",
    "c2_scale_weight = torch.tensor(weights['c2.weight'].q_scale())\n",
    "c2_scale_x = weights['c1.scale'].item()  # scale of input\n",
    "c2_scale_y = weights['c2.scale'].item()  # scale of output\n",
    "c2_multipler = c2_scale_weight.mul_(c2_scale_x / c2_scale_y)   # in-place to save memory, shape [C_out]\n",
    "\n",
    "c2_max_m = c2_multipler.abs().max().item()\n",
    "INT32_MAX = 2**31 - 1\n",
    "c2_R_max = math.floor(math.log2(INT32_MAX / c2_max_m))\n",
    "# clamp R to [0, 31] if you like\n",
    "c2_R = min(31, max(0, c2_R_max))\n",
    "# --- 4) Compute the integer multipliers M[o] ----------------------\n",
    "#   M[o] = round(m[o] * 2^R)\n",
    "c2_M = torch.round(c2_multipler * (2**c2_R)).to(torch.int32)  # Tensor[C_out]\n",
    "print(f\"Using R = {c2_R}\")\n",
    "print(\"First few fixed-point multipliers:\", c2_M)\n",
    "c2_M_vec = c2_M.repeat(weights['c2.weight'].shape[0])\n",
    "c1_maxpool_tensor = torch.tensor(c1_maxpool, dtype=torch.float32)\n",
    "# C_out, _, kH, kW = w_q.shape\n",
    "c2_conv2d_result_manual = int8_conv2d_integer_only(\n",
    "    c1_relu,\n",
    "    weights['c2.weight'],\n",
    "    c2_bias_int32, \n",
    "    weights['c1.zero_point'].item(), weights['c2.weight'].q_zero_point(), weights['c2.zero_point'],\n",
    "    M=c2_M_vec, R=c2_R,\n",
    "    stride=1,padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "b059b917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[83, 81, 79, 78, 79, 80, 80, 80, 81, 81],\n",
      "        [79, 78, 78, 77, 75, 75, 79, 80, 80, 81],\n",
      "        [81, 79, 78, 78, 77, 80, 86, 85, 81, 80],\n",
      "        [82, 80, 81, 83, 82, 84, 87, 85, 79, 79],\n",
      "        [81, 81, 81, 80, 78, 82, 85, 85, 78, 78],\n",
      "        [79, 78, 79, 78, 76, 80, 85, 83, 77, 78],\n",
      "        [78, 79, 80, 80, 79, 83, 86, 83, 77, 79],\n",
      "        [82, 82, 82, 81, 81, 85, 86, 81, 75, 80],\n",
      "        [83, 83, 83, 81, 82, 86, 85, 77, 75, 82],\n",
      "        [83, 83, 82, 81, 83, 86, 83, 74, 77, 84]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "print(c2_conv2d_result_manual[0,0,:,:])\n",
    "# torch.save(c2_conv2d_result_manual, f'{IMAGE_NAME}_c2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "b20e7e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "c2_maxpool = maxpool2d(c2_conv2d_result_manual.numpy(), kernel_size=2, stride=2)\n",
    "print(c2_maxpool.shape)\n",
    "torch.save(torch.tensor(c2_maxpool), f'{IMAGE_NAME}_pool_c2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "a0ef81d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "c2_relu = torch.clamp(torch.tensor(c2_maxpool), min=weights['c2.zero_point'])\n",
    "# torch.save(c2_relu, f'{IMAGE_NAME}_relu_c2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "c094bc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using R = 31\n",
      "First few fixed-point multipliers: tensor(8327137, dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gunaw\\AppData\\Local\\Temp\\ipykernel_16680\\2978365005.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  c2_maxpool_tensor = torch.tensor(c2_relu, dtype=torch.float32)\n",
      "C:\\Users\\gunaw\\AppData\\Local\\Temp\\ipykernel_16680\\613243976.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  quantized_clipped = torch.clip(torch.tensor(quantized_down), 0, 255)\n"
     ]
    }
   ],
   "source": [
    "c3_bias_int32 = torch.round(weights['c3.bias'] / (weights['c2.scale'].item()  * weights['c3.weight'].q_scale())).to(torch.int32) # convert bias to int32\n",
    "c3_scale_weight = torch.tensor(weights['c3.weight'].q_scale())\n",
    "c3_scale_x = weights['c2.scale'].item()  # scale of input\n",
    "c3_scale_y = weights['c3.scale'].item()  # scale of output\n",
    "c3_multipler = c3_scale_weight.mul_(c3_scale_x / c3_scale_y)   # in-place to save memory, shape [C_out]\n",
    "\n",
    "c3_max_m = c3_multipler.abs().max().item()\n",
    "INT32_MAX = 2**31 - 1\n",
    "c3_R_max = math.floor(math.log2(INT32_MAX / c3_max_m))\n",
    "# clamp R to [0, 31] if you like\n",
    "c3_R = min(31, max(0, c3_R_max))\n",
    "# --- 4) Compute the integer multipliers M[o] ----------------------\n",
    "#   M[o] = round(m[o] * 2^R)\n",
    "c3_M = torch.round(c3_multipler * (2**c3_R)).to(torch.int32)  # Tensor[C_out]\n",
    "print(f\"Using R = {c3_R}\")\n",
    "print(\"First few fixed-point multipliers:\", c3_M)\n",
    "c3_M_vec = c3_M.repeat(weights['c3.weight'].shape[0])\n",
    "c2_maxpool_tensor = torch.tensor(c2_relu, dtype=torch.float32)\n",
    "# C_out, _, kH, kW = w_q.shape\n",
    "c3_conv2d_result_manual = int8_conv2d_integer_only(\n",
    "    c2_maxpool_tensor,\n",
    "    weights['c3.weight'],\n",
    "    c3_bias_int32, \n",
    "    weights['c2.zero_point'].item(), weights['c3.weight'].q_zero_point(), weights['c3.zero_point'],\n",
    "    M=c3_M_vec, R=c3_R,\n",
    "    stride=1,padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "dea4d173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[74, 74, 74, 74, 78, 74, 74, 74, 74, 74, 74, 74, 74, 77, 74, 74, 74, 74,\n",
      "         74, 74, 76, 74, 74, 74, 74, 74, 74, 74, 74, 74, 75, 74, 83, 74, 74, 74,\n",
      "         74, 74, 79, 74, 74, 74, 74, 74, 78, 74, 78, 74, 78, 74, 74, 74, 74, 74,\n",
      "         74, 74, 74, 76, 74, 74, 74, 74, 74, 74, 74, 79, 74, 74, 74, 76, 74, 74,\n",
      "         74, 74, 74, 74, 74, 77, 78, 74, 75, 74, 74, 76, 74, 74, 74, 74, 74, 75,\n",
      "         74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 75, 74, 74, 74, 74,\n",
      "         74, 74, 74, 74, 79, 74, 74, 74, 76, 74, 74, 76]], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gunaw\\AppData\\Local\\Temp\\ipykernel_16680\\4183517818.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  c3_relu = torch.clamp(torch.tensor(c3_conv2d_result_manual_flatten), min=weights['c3.zero_point'])\n"
     ]
    }
   ],
   "source": [
    "c3_conv2d_result_manual_flatten = torch.flatten(c3_conv2d_result_manual).unsqueeze(0)\n",
    "c3_relu = torch.clamp(torch.tensor(c3_conv2d_result_manual_flatten), min=weights['c3.zero_point'])\n",
    "# torch.save(c3_relu, f'{IMAGE_NAME}_relu_c3.pt')\n",
    "print(c3_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "4e456d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def requantize_fbgemm(acc: int, M: int, R: int, zp: int) -> int:\n",
    "    prod = np.int64(acc) * np.int64(M)\n",
    "    # saturating rounding doubling high mul\n",
    "    nudge = np.int64(1) << 31\n",
    "    hi31  = int((prod + nudge) >> 32)\n",
    "    # now downshift by (31 - R)\n",
    "    scaled = hi31 >> (31 - R)\n",
    "    return max(0, min(255, scaled + zp))\n",
    "\n",
    "def quantized_linear(\n",
    "    x_q,                # torch.quint8 [N, in_features]\n",
    "    w_q,                # torch.qint8  [out_features, in_features]\n",
    "    bias_int32,         # torch.int32  [out_features]\n",
    "    x_zp, w_zp, y_zp,   # ints\n",
    "    M, R,                # Tensor[out_features], int shift\n",
    "    input_scale=0, output_scale=0,\n",
    "):\n",
    "    N, in_f = x_q.shape\n",
    "    out_f, _ = w_q.shape\n",
    "\n",
    "    # 1) subtract zero-points\n",
    "    x_int = x_q.int().to(torch.int32) - x_zp\n",
    "    w_int = w_q.int_repr().to(torch.int32) - w_zp  # per-channel if needed\n",
    "    w_scale = w_q.q_scale()\n",
    "    # 2) int32 matmul + bias\n",
    "    y_int32 = torch.zeros((N, out_f), dtype=torch.int32)\n",
    "    for n in range(N):\n",
    "        for o in range(out_f):\n",
    "            acc = torch.dot(x_int[n], w_int[o])   # int32\n",
    "            # print(acc.dtype)\n",
    "            acc = acc + bias_int32[o]             # int32\n",
    "            # 3) requantize\n",
    "            # saturating rounding, high‐mul, then shift:\n",
    "            prod = int(acc) * int(M.item())\n",
    "            # add rounding nudge\n",
    "            acc_q = (prod + (1 << (R - 1))) >> R\n",
    "            # print(acc_q)\n",
    "            # add output zero‐point and clamp\n",
    "            acc_q = acc_q + y_zp\n",
    "            y_int32[n, o] = max(0, min(255, acc_q))\n",
    "            y_int32[n, o] = requantize_fbgemm(acc_q,M=M,R=R,zp=y_zp)\n",
    "            # rounding to 8 bits\n",
    "            z_out  = torch.round((acc)*((input_scale*w_scale)/output_scale))\n",
    "            # z_out = saturate(z_out)\n",
    "            y_int32[n, o] = z_out\n",
    "\n",
    "    return y_int32.to(torch.uint8)\n",
    "\n",
    "def quantize_multiplier(real_scale):\n",
    "    if real_scale == 0:\n",
    "        return 0, 0\n",
    "    significand, exponent = np.frexp(real_scale)  # real_scale = s * 2^e, |s| in [0.5,1)\n",
    "    M = int(torch.round(significand * (1 << 31)))      # Q31 fixed-point\n",
    "    R = -exponent                                # right shift after the high-mul\n",
    "    return M, R\n",
    "\n",
    "# def requantize_fixed_point(src: np.ndarray, multiplier: int, right_shift: int, zero_point: int) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     src: int32 numpy array (accumulator values)\n",
    "#     multiplier: fixed-point multiplier (int)\n",
    "#     right_shift: number of bits to right-shift\n",
    "#     zero_point: output zero point (usually uint8)\n",
    "#     \"\"\"\n",
    "#     src = src.astype(np.int64)  # promote to avoid overflow in multiplication\n",
    "#     ab_64 = src * multiplier\n",
    "\n",
    "#     # Add rounding offset (nudge)\n",
    "#     nudge = 1 << max(0, right_shift - 1)\n",
    "#     ab_64_rounded = ab_64 + nudge\n",
    "\n",
    "#     # Perform requantization (right shift + zero point offset)\n",
    "#     quantized_down = zero_point + (ab_64_rounded >> right_shift)\n",
    "#     # Clip to uint8 range\n",
    "#     quantized_clipped = torch.clip(quantized_down, 0, 255)\n",
    "\n",
    "#     return quantized_clipped.to(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6310e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-5.5541e-02, -1.1212e-01,  1.4950e-01, -2.1750e-02, -7.9550e-02,\n",
       "        -2.6794e-02, -1.0098e-02, -3.7917e-02,  4.5647e-02,  1.2435e-01,\n",
       "         1.4925e-02, -5.4840e-02,  4.2010e-02, -1.4417e-01, -8.1558e-02,\n",
       "         2.1808e-01, -1.0489e-01,  5.7456e-02, -5.4745e-05, -1.3483e-02,\n",
       "        -9.8336e-02,  1.2907e-01, -2.0111e-02, -4.8898e-02, -1.3850e-01,\n",
       "         9.4622e-02,  2.5789e-01, -3.4394e-02,  7.4453e-02,  2.1444e-02,\n",
       "         1.6184e-01,  5.2490e-02,  1.2724e-01,  3.5700e-02,  1.1433e-01,\n",
       "        -8.9549e-03, -5.0973e-02, -2.3144e-02,  1.7422e-01, -7.0503e-02,\n",
       "        -7.1462e-02, -1.0133e-01, -1.1298e-01, -7.0165e-02, -2.4324e-02,\n",
       "         1.8525e-01, -1.2957e-01,  7.9836e-02,  1.8914e-01, -1.8761e-02,\n",
       "         1.2058e-01,  1.4163e-01, -6.8169e-02,  5.9301e-02,  2.9853e-02,\n",
       "         7.0051e-03, -8.2605e-02,  4.7995e-02, -1.1337e-01, -2.4698e-02,\n",
       "        -1.0141e-01,  2.1510e-01,  3.1197e-01, -1.5831e-01, -8.5254e-02,\n",
       "        -2.0909e-02,  4.8620e-02,  6.4252e-02, -1.7245e-01,  3.0351e-02,\n",
       "        -9.8979e-02,  1.8719e-01,  1.4742e-02, -2.1765e-01, -6.7101e-02,\n",
       "         3.9103e-02, -2.1605e-01,  4.6891e-02, -1.0909e-01, -6.4066e-02,\n",
       "         1.8722e-01,  1.6330e-01, -1.1230e-01,  4.1527e-02],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(profile=\"full\")\n",
    "weights['fc1._packed_params._packed_params'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "7a0f55af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def quantized_linear_manual(x_q, w_q, b_fp32, s_x, s_w, s_y, x_zp, w_zp, z_y):\n",
    "    \"\"\"\n",
    "    Simulates PyTorch quantized.Linear using int32 math + requantization\n",
    "\n",
    "    x_q:  [N, in_features]  (torch.quint8)\n",
    "    w_q:  [out_features, in_features]  (torch.qint8)\n",
    "    b_fp32: [out_features]  (torch.float32)\n",
    "    s_x, s_w, s_y: float scales\n",
    "    x_zp, w_zp, z_y: int zero-points\n",
    "    \"\"\"\n",
    "    x_int = x_q.int() - x_zp                 # [N, in_features], int32\n",
    "    w_int = w_q.int_repr().int() - w_zp      # [out_features, in_features], int32\n",
    "\n",
    "    # Step 1: int32 matmul\n",
    "    acc = torch.matmul(x_int, w_int.T)       # [N, out_features], int32\n",
    "\n",
    "    # Step 2: quantize bias (to match accumulator scale)\n",
    "    bias_int32 = torch.round(b_fp32 / (s_x * s_w)).to(torch.int32)\n",
    "\n",
    "    # Step 3: add bias\n",
    "    acc += bias_int32\n",
    "\n",
    "    # Step 4: requantization (fixed-point scale + shift)\n",
    "    real_scale = (s_x * s_w) / s_y\n",
    "    M, R = quantize_scale(real_scale)\n",
    "\n",
    "    # Step 5: fixed-point multiply + shift + zp + clamp\n",
    "    acc = requantize_fixed_point_tensor(acc, M, R, z_y)\n",
    "\n",
    "    return acc.to(torch.uint8)\n",
    "\n",
    "def quantize_scale(real_scale):\n",
    "    \"\"\"Convert real scale to fixed-point multiplier and shift\"\"\"\n",
    "    significand, exponent = np.frexp(real_scale)\n",
    "    M = int(torch.round(significand * (1 << 31)))   # Q31 multiplier\n",
    "    R = -exponent\n",
    "    return M, R\n",
    "\n",
    "def requantize_fixed_point_tensor(acc, M, R, z_y):\n",
    "    \"\"\"\n",
    "    Apply saturating rounding high-mul + shift and add output zp\n",
    "    acc: int32 tensor\n",
    "    M: int\n",
    "    R: int\n",
    "    z_y: output zp\n",
    "    \"\"\"\n",
    "    acc = acc.to(torch.int64)\n",
    "    prod = acc * M\n",
    "    # Apply rounding\n",
    "    prod += (1 << 30)\n",
    "    prod = prod >> 31  # this is saturating rounding high mul\n",
    "    # Apply right shift\n",
    "    prod = prod >> (R - 1)\n",
    "    # Add zp and clamp\n",
    "    acc_q = prod + z_y\n",
    "    acc_q = torch.clamp(acc_q, 0, 255)\n",
    "    return acc_q.to(torch.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "8bed35f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[67, 67, 67, 73, 70, 67, 72, 67, 70, 67, 67, 67, 67, 67, 67, 67, 67, 67,\n",
      "         67, 68, 71, 73, 67, 79, 77, 67, 74, 67, 78, 75, 67, 83, 67, 67, 67, 67,\n",
      "         67, 67, 67, 67, 75, 81, 67, 85, 67, 67, 67, 74, 81, 67, 67, 71, 67, 67,\n",
      "         67, 69, 67, 77, 67, 67, 67, 67, 67, 67, 67, 70, 80, 68, 67, 67, 73, 67,\n",
      "         67, 67, 67, 79, 67, 67, 67, 71, 67, 67, 67, 67]], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gunaw\\AppData\\Local\\Temp\\ipykernel_16680\\238433758.py:37: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  significand, exponent = np.frexp(real_scale)\n",
      "C:\\Users\\gunaw\\AppData\\Local\\Temp\\ipykernel_16680\\865937651.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fc1_relu = torch.clamp(torch.tensor(y_manual), min=weights['fc1.zero_point'])\n"
     ]
    }
   ],
   "source": [
    "fc1_bias_fp32 = weights['fc1._packed_params._packed_params'][1]\n",
    "fc1_scale_in = weights['c3.scale']\n",
    "fc1_scale_weight = weights['fc1._packed_params._packed_params'][0].q_scale()\n",
    "# Run manual quantized linear\n",
    "y_manual = quantized_linear_manual(\n",
    "    c3_relu, weights['fc1._packed_params._packed_params'][0], fc1_bias_fp32,\n",
    "    s_x=fc1_scale_in, s_w=fc1_scale_weight, s_y=weights['fc1.scale'],\n",
    "    x_zp=weights['c3.zero_point'].item(), w_zp=weights['fc1._packed_params._packed_params'][0].q_zero_point(), z_y=weights['fc1.zero_point'].item()\n",
    ")\n",
    "\n",
    "fc1_relu = torch.clamp(torch.tensor(y_manual), min=weights['fc1.zero_point'])\n",
    "print(fc1_relu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "5eab5b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(fc1_relu, f'{IMAGE_NAME}_fc1_relu.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "83f77046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 57,  70,  69,  77,  68,  73,  43, 105,  64,  80]], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gunaw\\AppData\\Local\\Temp\\ipykernel_16680\\238433758.py:37: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  significand, exponent = np.frexp(real_scale)\n"
     ]
    }
   ],
   "source": [
    "fc2_bias_fp32 = weights['fc2._packed_params._packed_params'][1]\n",
    "fc2_scale_in = weights['fc1.scale']\n",
    "fc2_scale_weight = weights['fc2._packed_params._packed_params'][0].q_scale()\n",
    "# Run manual quantized linear\n",
    "fc2_manual = quantized_linear_manual(\n",
    "    fc1_relu, weights['fc2._packed_params._packed_params'][0], fc2_bias_fp32,\n",
    "    s_x=fc2_scale_in, s_w=fc2_scale_weight, s_y=weights['fc2.scale'],\n",
    "    x_zp=weights['fc1.zero_point'].item(), w_zp=weights['fc2._packed_params._packed_params'][0].q_zero_point(), z_y=weights['fc2.zero_point'].item()\n",
    ")\n",
    "print(fc2_manual)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
