{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4a383e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "# SINCE THE MNIST DATASET IS IN 28X28, PADDING OF 2 IS PUT ON C1 LAYER\n",
    "class LeNet5_Dict_Out(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5_Dict_Out,self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        self.c1 = nn.Conv2d(in_channels=1,out_channels=6,kernel_size=5,stride=1,padding=2)\n",
    "        self.c2 = nn.Conv2d(in_channels=6,out_channels=16,kernel_size=5,stride=1,padding=0)\n",
    "        self.c3 = nn.Conv2d(in_channels=16,out_channels=120,kernel_size=5,stride=1,padding=0)\n",
    "        # self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        # self.fc0 = nn.Linear(in_features=400,out_features=120)\n",
    "        self.fc1 = nn.Linear(in_features=120,out_features=84)\n",
    "        self.fc2 = nn.Linear(in_features=84,out_features=10)\n",
    "\n",
    "    def forward(self, img):\n",
    "        out_dict = {}\n",
    "        x = self.quant(img)\n",
    "        # in 1x32x32 out 6x28x28\n",
    "        x = self.c1(x)\n",
    "        out_dict['c1'] = x\n",
    "        # in 6x28x28 out 6x14x14\n",
    "        # replace tanh with relu\n",
    "        x = self.max_pool(x)\n",
    "        out_dict['pool_c1'] = x\n",
    "        x = self.relu(x)\n",
    "        out_dict['relu_c1'] = x\n",
    "        # in 6x14x14 out 16x10x10\n",
    "        x = self.c2(x)\n",
    "        out_dict['c2'] = x\n",
    "        # in 16x10x10 out 16x5x5\n",
    "        # replace tanh with relu\n",
    "        x = self.relu(self.max_pool(x))\n",
    "        out_dict['pool_c2'] = x\n",
    "        # # in 16x5x5 out 400x1x1\n",
    "        # x = torch.flatten(x,1)\n",
    "        # x = self.tanh(self.fc0(x))\n",
    "        # in 16x5x5 out 120x1x1\n",
    "        # replace tanh with relu\n",
    "        x = self.relu(self.c3(x))\n",
    "        x = torch.flatten(x,1)\n",
    "        out_dict['c3'] = x.int_repr()\n",
    "        # replace tanh with relu\n",
    "        x= self.fc1(x)\n",
    "        out_dict['fc1'] = x\n",
    "        x = self.relu(x)\n",
    "        out_dict['fc1_relu'] = x\n",
    "        x = self.fc2(x)\n",
    "        out_dict['fc2'] = x\n",
    "        x = self.dequant(x)\n",
    "        return x, out_dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9bd1d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "MODEL_NAME = \"quantized_lenet5_mnist_20250703_1003.pth\"\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3dd96855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST mean: 0.132515\n",
      "MNIST std:  0.310480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gunaw\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:244: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gunaw\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\ao\\quantization\\utils.py:408: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LeNet5_Dict_Out(\n",
       "  (quant): Quantize(scale=tensor([0.0256]), zero_point=tensor([17]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       "  (c1): QuantizedConv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), scale=0.13094060122966766, zero_point=68, padding=(2, 2))\n",
       "  (c2): QuantizedConv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), scale=0.33193832635879517, zero_point=83)\n",
       "  (c3): QuantizedConv2d(16, 120, kernel_size=(5, 5), stride=(1, 1), scale=0.5925276279449463, zero_point=74)\n",
       "  (relu): ReLU()\n",
       "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): QuantizedLinear(in_features=120, out_features=84, scale=0.5568221807479858, zero_point=67, qscheme=torch.per_tensor_affine)\n",
       "  (fc2): QuantizedLinear(in_features=84, out_features=10, scale=1.0586904287338257, zero_point=84, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "# device-agnostic setup\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# load test dataset\n",
    "BATCH_SIZE = 32\n",
    "test_dataset  = torchvision.datasets.MNIST(root='./data', train=False, download=False, transform=transforms.ToTensor())\n",
    "imgs = torch.stack([img for img, _ in test_dataset], dim=0)\n",
    "mean = imgs.view(1, -1).mean(dim=1)    # or imgs.mean()\n",
    "std = imgs.view(1, -1).std(dim=1)     # or imgs.std()\n",
    "print(f\"MNIST mean: {mean.item():.6f}\")\n",
    "print(f\"MNIST std:  {std.item():.6f}\")\n",
    "# create Transformation (converting from Image class to Tensor and normalize)\n",
    "mnist_transforms = transforms.Compose([transforms.ToTensor(),])\n",
    "                                    #    transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "test_dataset  = torchvision.datasets.MNIST(root='./data', train=False, download=False, transform=mnist_transforms)\n",
    "class_names = test_dataset.classes\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "# Step 1: Re-create model and set QAT config\n",
    "model_fp32 = LeNet5_Dict_Out()\n",
    "model_fp32.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "torch.quantization.prepare_qat(model_fp32 , inplace=True)\n",
    "\n",
    "# Step 2: Convert to quantized version\n",
    "net = torch.quantization.convert(model_fp32 .eval(), inplace=False)\n",
    "# Step 3: Load quantized weights\n",
    "net.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "\n",
    "\n",
    "test_loss, test_acc = 0, 0\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce14d93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  75.,\n",
      "          168., 168., 168., 168.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0., 168., 255., 255., 255., 255., 255.,\n",
      "          255., 255., 255.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0., 168., 255., 255., 255., 255., 255., 255., 168.,\n",
      "          168.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0., 255., 255., 255., 168.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0., 255., 255.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0., 255., 255., 255., 168.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          255., 255., 255., 255., 255., 255.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 255.,\n",
      "          255., 255., 255., 255., 255., 255., 255.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 255.,\n",
      "          255., 255.,   0.,   0., 255., 255., 255.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0., 255., 255., 255.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0., 255., 255., 168.,   0.,   0.,\n",
      "            0.,   0.,  75., 168., 255., 255., 255.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0., 255., 255., 255., 255., 255.,\n",
      "          255., 255., 255., 255., 255., 255.,  75.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0., 168., 255., 255., 255., 255.,\n",
      "          255., 255., 255., 255., 255.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 168.,\n",
      "          168., 168., 168.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.]]])\n",
      "tensor([[[17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "          17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "          17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "          17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "          17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "          17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "          17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "          17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "          17, 17, 17, 17, 29, 43, 43, 43, 43, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 43,\n",
      "          56, 56, 56, 56, 56, 56, 56, 56, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 43, 56, 56,\n",
      "          56, 56, 56, 56, 43, 43, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 56, 56,\n",
      "          56, 43, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 56, 56, 17, 17,\n",
      "          17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 56, 56, 56, 43, 17,\n",
      "          17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 56, 56, 56, 56, 56, 56,\n",
      "          17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 56, 56, 56, 56, 56, 56, 56,\n",
      "          56, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 56, 56, 56, 17, 17, 56, 56,\n",
      "          56, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 56, 56,\n",
      "          56, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 56, 56, 43, 17, 17, 17, 17, 29, 43, 56, 56,\n",
      "          56, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56,\n",
      "          29, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 43, 56, 56, 56, 56, 56, 56, 56, 56, 56, 17,\n",
      "          17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 43, 43, 43, 43, 17, 17, 17,\n",
      "          17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "          17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "          17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "          17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "          17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "          17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "          17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "         [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "          17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17]]], dtype=torch.uint8)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input shape must be `(N, C, H, W)`!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(net\u001b[38;5;241m.\u001b[39mquant(img_tensor)\u001b[38;5;241m.\u001b[39mint_repr())\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# img_tensor.unsqueeze_(0)  # shape: [1, 1, 28, 28]\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m output, out_dict \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# print(output)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# print(out_dict['c1'])\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gunaw\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gunaw\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[63], line 24\u001b[0m, in \u001b[0;36mLeNet5_Dict_Out.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant(img)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# in 1x32x32 out 6x28x28\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m out_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# in 6x28x28 out 6x14x14\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# replace tanh with relu\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gunaw\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gunaw\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\gunaw\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:588\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;66;03m# Temporarily using len(shape) instead of ndim due to JIT issue\u001b[39;00m\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/23890\u001b[39;00m\n\u001b[0;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m--> 588\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape must be `(N, C, H, W)`!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    590\u001b[0m         _reversed_padding_repeated_twice \u001b[38;5;241m=\u001b[39m _reverse_repeat_padding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding)\n",
      "\u001b[1;31mValueError\u001b[0m: Input shape must be `(N, C, H, W)`!"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "IMAGE_NAME = \"img_01.png\"\n",
    "IMAGE_PATH = Path(Path.cwd() / \"results\" / \"images\" / IMAGE_NAME)\n",
    "\n",
    "weights = torch.load(MODEL_SAVE_PATH)\n",
    "img = Image.open(IMAGE_PATH).convert('L')  # 'L' for grayscale\n",
    "# plt.imshow(img, cmap='gray')\n",
    "img_tensor = mnist_transforms(img)\n",
    "print(img_tensor*255)\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "print(net.quant(img_tensor).int_repr())\n",
    "img_tensor.unsqueeze_(0)  # shape: [1, 1, 28, 28]\n",
    "output, out_dict = net(img_tensor)\n",
    "# print(output)\n",
    "# print(out_dict['c1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce30941a",
   "metadata": {},
   "source": [
    "## CHECK FOR C1 DIFFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8705ec30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 65],\n",
      "        [67, 67, 67, 67, 64, 65, 68, 66, 64, 65],\n",
      "        [67, 67, 67, 64, 64, 67, 68, 68, 70, 73],\n",
      "        [67, 67, 65, 65, 69, 73, 70, 73, 77, 75]], dtype=torch.uint8)\n",
      "tensor(260278)\n"
     ]
    }
   ],
   "source": [
    "c1_pytorch = out_dict['c1']\n",
    "c1_manual = torch.load('img_01.png_c1.pt')\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "print(c1_pytorch[0,0,0:10,0:10].int_repr())\n",
    "print(torch.sum(c1_manual - c1_pytorch.int_repr())) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300c7fe8",
   "metadata": {},
   "source": [
    "## CHECK FOR POOLING of c1 DIFFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bef68181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 68, 67, 72, 72, 72, 72, 71, 67, 67, 67, 67],\n",
      "        [67, 67, 73, 73, 77, 77, 74, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 71, 73, 72, 62, 66, 69, 69, 69, 70, 67, 67, 67, 67],\n",
      "        [67, 67, 62, 68, 70, 71, 70, 67, 67, 68, 66, 67, 67, 67],\n",
      "        [67, 67, 68, 71, 68, 67, 67, 67, 68, 66, 66, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 65, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 66, 68, 66, 68, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 68, 69, 65, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 69, 67, 67, 68, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 69, 63, 68, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 69, 67, 66, 68, 67, 67, 67, 67]],\n",
      "       dtype=torch.uint8)\n",
      "tensor([[67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 69, 68],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 68, 72, 74, 76, 75, 73, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 70, 74, 74, 73, 67, 61, 67, 68],\n",
      "        [67, 67, 67, 67, 67, 69, 75, 69, 66, 68, 70, 70, 71, 68],\n",
      "        [67, 67, 67, 67, 68, 74, 70, 66, 73, 73, 68, 67, 67, 67],\n",
      "        [67, 67, 67, 69, 72, 70, 59, 68, 68, 65, 67, 67, 67, 67],\n",
      "        [67, 67, 70, 70, 68, 74, 78, 74, 69, 66, 67, 67, 67, 67],\n",
      "        [67, 67, 69, 68, 65, 69, 69, 63, 66, 69, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 68, 69, 66, 67, 71, 70, 69, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 68, 70, 69, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67],\n",
      "        [67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67]],\n",
      "       dtype=torch.uint8)\n",
      "tensor(64301)\n"
     ]
    }
   ],
   "source": [
    "c1_pool = out_dict['pool_c1']\n",
    "print(c1_pool[0,0,:,:].int_repr())\n",
    "c1_pool_manual = torch.load('img_01.png_pool_c1.pt')\n",
    "print(c1_pool_manual[0,0,:,:])\n",
    "print(torch.sum(c1_pool_manual - c1_pool.int_repr()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a3a0252e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(34876)\n"
     ]
    }
   ],
   "source": [
    "relu_c1 = out_dict['relu_c1']\n",
    "relu_c1_manual = torch.load('img_01.png_relu_c1.pt')\n",
    "print(torch.sum(relu_c1_manual-relu_c1.int_repr()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ae3692b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 10, 10])\n",
      "tensor([[83, 81, 79, 78, 79, 80, 80, 80, 81, 81],\n",
      "        [79, 78, 78, 77, 75, 75, 79, 80, 80, 81],\n",
      "        [81, 79, 78, 78, 77, 80, 86, 85, 81, 80],\n",
      "        [82, 80, 81, 83, 82, 84, 87, 85, 79, 79],\n",
      "        [81, 81, 81, 80, 78, 82, 85, 85, 78, 78],\n",
      "        [79, 78, 79, 78, 76, 80, 85, 83, 77, 78],\n",
      "        [78, 79, 80, 80, 79, 83, 86, 83, 77, 79],\n",
      "        [82, 82, 82, 81, 81, 85, 86, 81, 75, 80],\n",
      "        [83, 83, 83, 81, 82, 86, 85, 77, 75, 82],\n",
      "        [83, 83, 82, 81, 83, 86, 83, 74, 77, 84]], dtype=torch.uint8)\n",
      "tensor(177352)\n"
     ]
    }
   ],
   "source": [
    "conv_c2 = out_dict['c2']\n",
    "print(conv_c2.shape)\n",
    "print(conv_c2[0,0,:,:].int_repr())\n",
    "conv_c2_manual = torch.load('img_01.png_c2.pt')\n",
    "print(torch.sum(conv_c2.int_repr() - conv_c2_manual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1901f344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(22584)\n"
     ]
    }
   ],
   "source": [
    "c2_pool_relu = out_dict['pool_c2']\n",
    "c2_pool_relu_manual = torch.load('img_01.png_relu_c2.pt')\n",
    "print(torch.sum(c2_pool_relu_manual - c2_pool_relu.int_repr()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f27a4a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[74, 74, 74, 74, 74, 80, 74, 80, 74, 74, 74, 74, 80, 74, 74, 74, 74, 74,\n",
      "         74, 78, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74,\n",
      "         74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 83, 74, 74, 74, 78,\n",
      "         74, 74, 77, 74, 74, 74, 78, 74, 81, 74, 74, 74, 74, 74, 74, 74, 74, 74,\n",
      "         74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74,\n",
      "         74, 74, 74, 74, 78, 74, 74, 74, 74, 74, 74, 74, 74, 75, 74, 74, 74, 74,\n",
      "         74, 74, 74, 74, 74, 74, 77, 74, 74, 74, 77, 78]], dtype=torch.uint8)\n",
      "tensor(4862)\n"
     ]
    }
   ],
   "source": [
    "c3_conv_relu = out_dict['c3']\n",
    "c3_conv_relu_manual = torch.load('img_01.png_relu_c3.pt')\n",
    "print(c3_conv_relu_manual)\n",
    "print(torch.sum(c3_conv_relu_manual - c3_conv_relu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "877eec5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[67, 67, 67, 70, 68, 67, 70, 67, 69, 67, 67, 67, 67, 67, 67, 67, 67, 67,\n",
      "         67, 68, 69, 70, 67, 73, 72, 67, 71, 67, 73, 71, 67, 75, 67, 67, 67, 67,\n",
      "         67, 67, 67, 67, 71, 74, 67, 76, 67, 67, 67, 71, 74, 67, 67, 69, 67, 67,\n",
      "         67, 68, 67, 72, 67, 67, 67, 67, 67, 67, 67, 69, 74, 68, 67, 67, 70, 67,\n",
      "         67, 67, 67, 73, 67, 67, 67, 69, 67, 67, 67, 67]], dtype=torch.uint8)\n",
      "tensor([[  2,   3,   0,  11, 255,   0, 253,   0, 254,   0,   0,   0,   0,   6,\n",
      "           0,   7,   0,   0,   8,   5,   8, 255,   0, 250, 251,   0, 252,   0,\n",
      "         250, 252,  16, 248,   0,  13,   0,   0,   0,   0,  14,   0, 252, 250,\n",
      "           0, 247,   8,  13,   0, 252, 249,   0,   0, 254,   8,   4,  11, 255,\n",
      "           0, 251,   0,   0,   0,  11,   0,   7,   0, 254, 249, 255,  12,   8,\n",
      "           5,   0,   0,   5,   0, 254,   0,  18,   0, 254,   0,   7,   0,   0]],\n",
      "       dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "fc1_result = out_dict['fc1']\n",
    "# print(fc1_result.int_repr())\n",
    "fc1_relu = out_dict['fc1_relu']\n",
    "print(fc1_relu.int_repr())\n",
    "fc1_relu_manual = torch.load('img_01.png_fc1_relu.pt')\n",
    "print(fc1_relu_manual - fc1_relu.int_repr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8427d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from torchvision.utils import save_image\n",
    "# torch.manual_seed(42)  # setting random seed\n",
    "\n",
    "# # Create output folders\n",
    "# output_img_dir = Path(\"results/images\")\n",
    "# output_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "# output_txt_path = Path(\"results/results.txt\")\n",
    "\n",
    "# # Clear or create the results.txt file\n",
    "# with open(output_txt_path, 'w') as f:\n",
    "#     f.write(\"idx, ground_truth, predicted\\n\")\n",
    "\n",
    "# rows, cols = 2, 6\n",
    "# fig = plt.figure(figsize=(12, 4))\n",
    "# for i in range(1, (rows * cols) + 1):\n",
    "#     random_idx = torch.randint(0, len(test_dataset), size=[1]).item()\n",
    "#     img, label_gt = test_dataset[random_idx]\n",
    "#     print(random_idx, img.shape, label_gt)\n",
    "#     # print(img)\n",
    "#     img_tensor = img.unsqueeze(dim=0).to(device)\n",
    "#     # torch.save(img_tensor, f'img_{i:02d}.pt')  # Save the tensor for later use\n",
    "#     # get the stuff with the highest confidence(?)\n",
    "#     # label_pred = torch.argmax(net(img_tensor)).item()\n",
    "#     with torch.no_grad():\n",
    "#         print(net.quant(img_tensor).int_repr())\n",
    "\n",
    "#         output, out_dict = net(img_tensor)\n",
    "#         prob = torch.softmax(output, dim=1)[0]  # shape: [10]\n",
    "#         label_pred = torch.argmax(prob).item()\n",
    "#     # === Save image to PNG ===\n",
    "#     img_filename = f\"img_{i:02d}.png\"\n",
    "#     save_image(img, output_img_dir / img_filename)\n",
    "\n",
    "#     # === Write result to text file ===\n",
    "#     with open(output_txt_path, 'a') as f:\n",
    "#         # f.write(f\"{img_filename}, {class_names[label_gt]}, {class_names[label_pred]}\\n\")\n",
    "#         prob_str = \", \".join([f\"{class_names[i]}: {prob[i]:.4f}\" for i in range(10)])\n",
    "#         f.write(f\"{img_filename}, GT: {class_names[label_gt]}, Pred: {class_names[label_pred]} → [{prob_str}]\\n\")    # === Plot image ===\n",
    "#     fig.add_subplot(rows, cols, i)\n",
    "#     img_display = img.permute(1, 2, 0)  # CxHxW → HxWxC\n",
    "#     plt.imshow(img_display, cmap='gray')\n",
    "#     color = 'g' if label_pred == label_gt else 'r'\n",
    "#     plt.title(class_names[label_pred], color=color)\n",
    "#     plt.axis('off')\n",
    "#     break\n",
    "# # print(out_dict)\n",
    "# # torch.save(out_dict['c1'], f'img_01.png_c1_pytorch.pt')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "77a58919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[77, 80, 80, 82, 80, 81, 73, 90, 79, 83]], dtype=torch.uint8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LeNet5_Dict_Out(\n",
       "  (quant): Quantize(scale=tensor([0.0256]), zero_point=tensor([17]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       "  (c1): QuantizedConv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), scale=0.13094060122966766, zero_point=68, padding=(2, 2))\n",
       "  (c2): QuantizedConv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), scale=0.33193832635879517, zero_point=83)\n",
       "  (c3): QuantizedConv2d(16, 120, kernel_size=(5, 5), stride=(1, 1), scale=0.5925276279449463, zero_point=74)\n",
       "  (relu): ReLU()\n",
       "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): QuantizedLinear(in_features=120, out_features=84, scale=0.5568221807479858, zero_point=67, qscheme=torch.per_tensor_affine)\n",
       "  (fc2): QuantizedLinear(in_features=84, out_features=10, scale=1.0586904287338257, zero_point=84, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc2_result = out_dict['fc2']\n",
    "print(fc2_result.int_repr())\n",
    "net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
